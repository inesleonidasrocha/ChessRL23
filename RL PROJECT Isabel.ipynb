{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Final Project \n",
    "\n",
    "Welcome to your Reinforcement Learning project focused on developing an RL agent capable of playing chess at a strategic level. Chess has long been considered a benchmark for measuring AI capabilities, and this project aims to leverage the power of RL to create an intelligent agent that can make optimal decisions in complex chess positions. By combining the principles of reinforcement learning with the rich strategic domain of chess, you will explore new approaches to create the most effective chess player.\n",
    "\n",
    "## Project Objectives:\n",
    "\n",
    "* Train an RL agent to play chess: The primary objective of this project is to develop an RL agent that can play chess at a high level of proficiency. The agent should be capable of evaluating chess positions and making strategic decisions.\n",
    "\n",
    "* Optimize decision-making using RL algorithms: Explore different RL algorithms, as seen in class, to train the agent. Compare and analise their effectiveness in learning and decision-making capabilities in the context of chess.\n",
    "\n",
    "* Use a challenging chess environment: Use a comprehensive environment for the agent to interact with, representing the rules and dynamics of chess. This environment will provide a realistic and challenging setting for the agent's training and evaluation.\n",
    "\n",
    "* Evaluate and benchmark performance: Assess the performance of the RL agent against different benchmarks from existing chess engines. You will compare your agent's performance to established chess engines to measure progress and identify areas for improvement.\n",
    "\n",
    "\n",
    "### Extra Objectives:\n",
    "\n",
    "* Investigate transfer learning and generalization: Explore techniques for transfer learning to leverage knowledge acquired in related domains or from pre-training on large chess datasets. Investigate the agent's ability to generalize its knowledge.\n",
    "\n",
    "* Enhance interpretability and analysis: Develop methods to analise the agent's decision-making process and provide insights into its strategic thinking. Investigate techniques to visualize the agent's evaluation of chess positions and understand its reasoning behind specific moves."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Play Chess! \n",
    "\n",
    "As you know [Chess](https://en.wikipedia.org/wiki/Chess) is a board game for two players, called White and Black, each controlling an army of chess pieces in their color, with the objective to checkmate the opponent's king.\n",
    "\n",
    "Chess is an abstract strategy game that involves no hidden information and no use of dice or cards. It is played on a chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. White moves first, followed by Black. Checkmating the opponent's king involves putting the king under immediate attack (in \"check\") whereby there is no way for it to escape.\n",
    "\n",
    "\n",
    "![](Images/CHESS_MOVES.PNG)\n",
    "\n",
    "* The king moves one square in any direction. There is also a special move called castling that involves moving the king and a rook. The king is the most valuable piece â€” attacks on the king must be immediately countered, and if this is impossible, the game is immediately lost.\n",
    "* A rook can move any number of squares along a rank or file, but cannot leap over other pieces. Along with the king, a rook is involved during the king's castling move.\n",
    "* A bishop can move any number of squares diagonally, but cannot leap over other pieces.\n",
    "* A queen combines the power of a rook and bishop and can move any number of squares along a rank, file, or diagonal, but cannot leap over other pieces.\n",
    "* A knight moves to any of the closest squares that are not on the same rank, file, or diagonal. (Thus the move forms an \"L\"-shape: two squares vertically and one square horizontally, or two squares horizontally and one square vertically.) The knight is the only piece that can leap over other pieces.\n",
    "* A pawn can move forward to the unoccupied square immediately in front of it on the same file, or on its first move it can advance two squares along the same file, provided both squares are unoccupied (black dots in the diagram). A pawn can capture an opponent's piece on a square diagonally in front of it by moving to that square (black crosses). It cannot capture a piece while advancing along the same file. A pawn has two special moves: the en passant capture and promotion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The [Environment](https://github.com/iamlucaswolf/gym-chess)\n",
    "\n",
    "The environment gym-chess provides OpenAI Gym environments for the game of Chess. It comes with an implementation of the board and move encoding used in AlphaZero. \n",
    "\n",
    "Please install it using the command: \n",
    "\n",
    "`pip install gym-chess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import gym\n",
    "import gym_chess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Two player's game\n",
    "\n",
    "As you know chess is played by two players, as such the gym-chess environment gives you access to both players actions in a sequential matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WHITE_PLAYER_POLICY(env, state):\n",
    "    legal_actions = env.legal_actions\n",
    "    action = np.random.choice(legal_actions)\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def BLACK_PLAYER_POLICY(env, state):\n",
    "    legal_actions = env.legal_actions\n",
    "    action = np.random.choice(legal_actions)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"ChessAlphaZero-v0\"\n",
    ")  # We will use Alpha Zero's numenclature for the actions encodings\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "counter = 0  # Since each step represents a play in a chess game we are going to store the number of steps associated to the episode/game\n",
    "\n",
    "while not done:\n",
    "    if (\n",
    "        counter % 2 == 0\n",
    "    ):  # If the step number is even, this means that it is the WHITE player's turn\n",
    "        action = WHITE_PLAYER_POLICY(env, state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "    else:  # If the step number is not even, aka odd, this means that it is the BLACK player's turn\n",
    "        action = BLACK_PLAYER_POLICY(env, state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "print(reward)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The agent receives a reward of +1 when the white player makes a winning move, and a reward of -1 when the black player makes a winning move. \n",
    "\n",
    "All other rewards are zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluationg your agent with [Stockfish](https://github.com/zhelyabuzhsky/stockfish)\n",
    "\n",
    "In order to have a good enough idea that our agent is actually playing well we need a benchmarkable opponent.\n",
    "\n",
    "As such we need to install stockfish a free and open-source chess engine. Stockfish has consistently ranked first or near the top of most chess-engine rating lists and, as of April 2023, is the strongest CPU chess engine in the world.\n",
    "\n",
    "`pip install stockfish`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stockfish in c:\\users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages (3.28.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stockfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: numpy\n",
      "Version: 1.23.4\n",
      "Summary: NumPy is the fundamental package for array computing with Python.\n",
      "Home-page: https://www.numpy.org\n",
      "Author: Travis E. Oliphant et al.\n",
      "Author-email: \n",
      "License: BSD\n",
      "Location: c:\\users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: ale-py, contourpy, gym, gymnasium, h5py, imageio, jax, jax-jumpy, Keras-Applications, Keras-Preprocessing, matplotlib, ml-dtypes, moviepy, mujoco, mujoco-py, opencv-python, opt-einsum, pandas, scikit-learn, scipy, seaborn, tensorboard, tensorflow-intel, torchtext\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stockfish import Stockfish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StockFish has a python api as seen above, nevertheless the engine still needs to be downloaded [here](https://stockfishchess.org/download/) and used in the path.\n",
    "\n",
    "NOTE: You were given an engine already in moodle, nevertheless different computer systems (Windows, Mac, Ubuntu) might require other Stockfish engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stockfish_path = \"C:/Users/isabe/Desktop/RL/Project/stockfish_15.1_win_x64_avx2/stockfish-windows-2022-x86-64-avx2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions bellow generate episodes/games for a WHITE or BLACK Pieces Scenario respectively. We store the outcome of the episode (win/draw/loss) and the number of steps taken.\n",
    "\n",
    "#### Notice how the AGENT_POLICY function is used it recieves as inputs the env and the current state.\n",
    "`action = AGENT_POLICY(env, state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_WHITE_scenario(Stockfish_path, AGENT_POLICY):\n",
    "    env = gym.make(\n",
    "        \"ChessAlphaZero-v0\"\n",
    "    )  # We will use Alpha Zero's numenclature for the actions encodings\n",
    "    stockfish = Stockfish(Stockfish_path)\n",
    "    stockfish.set_elo_rating(\n",
    "        100\n",
    "    )  # Default \"skill\" level is 1350, higher will increase the skill of stockfish \"player\". See more at https://en.wikipedia.org/wiki/Elo_rating_system\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    counter = 0  # Since each step represents a play in a chess game we are going to store the number of steps associated to the episode/game\n",
    "\n",
    "    while not done:\n",
    "        if (\n",
    "            counter % 2 == 0\n",
    "        ):  # If the step number is pair, this means that it is the WHITE player's turn\n",
    "            action = AGENT_POLICY(env, state)\n",
    "            decoded_action = str(env.decode(action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        else:  # If the step number is not pair, aka odd, this means that it is the BLACK player's turn\n",
    "            decoded_action = stockfish.get_best_move()\n",
    "            action = env.encode(chess.Move.from_uci(decoded_action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return reward, np.ceil(counter / 2)\n",
    "\n",
    "\n",
    "def generate_BLACK_scenario(Stockfish_path, AGENT_POLICY):\n",
    "    env = gym.make(\n",
    "        \"ChessAlphaZero-v0\"\n",
    "    )  # We will use Alpha Zero's numenclature for the actions encodings\n",
    "    stockfish = Stockfish(Stockfish_path)\n",
    "    stockfish.set_elo_rating(\n",
    "        100\n",
    "    )  # Default \"skill\" level is 1350, higher will increase the skill of stockfish \"player\". See more at https://en.wikipedia.org/wiki/Elo_rating_system\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    counter = 0  # Since each step represents a play in a chess game we are going to store the number of steps associated to the episode/game\n",
    "\n",
    "    while not done:\n",
    "        if (\n",
    "            counter % 2 == 1\n",
    "        ):  # If the step number is not even, aka odd, this means that it is the BLACK player's turn\n",
    "            action = AGENT_POLICY(env, state)\n",
    "            decoded_action = str(env.decode(action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        else:  # If the step number is even, this means that it is the WHITE player's turn\n",
    "            decoded_action = stockfish.get_best_move()\n",
    "            action = env.encode(chess.Move.from_uci(decoded_action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return reward, np.ceil(counter / 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function bellow a visualization is produced from the bechmarks made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AGENT_EVALUATION(Stockfish_path, WHITE_PLAYER_POLICY, BLACK_PLAYER_POLICY, n_evaluations=100): #changed\n",
    "    results_list = []\n",
    "\n",
    "    for evaluation_number in tqdm(range(n_evaluations)):\n",
    "        generate_episode = generate_WHITE_scenario\n",
    "\n",
    "        reward, n_steps = generate_episode(Stockfish_path, WHITE_PLAYER_POLICY) #changed\n",
    "\n",
    "        if reward == 1:\n",
    "            result = \"VICTORY\"\n",
    "        elif reward == 0:\n",
    "            result = \"DRAW\"\n",
    "        else:\n",
    "            result = \"LOSS\"\n",
    "\n",
    "        results_list.append([\"WHITE\", result, n_steps])\n",
    "\n",
    "        generate_episode = generate_BLACK_scenario\n",
    "\n",
    "        reward, n_steps = generate_episode(Stockfish_path, BLACK_PLAYER_POLICY) #changed\n",
    "\n",
    "        if reward == -1:\n",
    "            result = \"VICTORY\"\n",
    "        elif reward == 0:\n",
    "            result = \"DRAW\"\n",
    "        else:\n",
    "            result = \"LOSS\"\n",
    "\n",
    "        results_list.append([\"BLACK\", result, n_steps])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        results_list, columns=[\"AGENT COLOR\", \"OUTCOME\", \"N STEPS\"]\n",
    "    ).astype(\"int\", errors=\"ignore\")\n",
    "\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "    results_group = (\n",
    "        df.groupby([\"AGENT COLOR\", \"OUTCOME\"])\n",
    "        .count()\n",
    "        .rename(columns={\"N STEPS\": \"GAMES\"})\n",
    "    )\n",
    "\n",
    "    n_games = results_group.sum()[0]\n",
    "\n",
    "    results_group = (2 * 100 * results_group / (n_games)).astype(\"int\")\n",
    "\n",
    "    viz_df = (\n",
    "        results_group.reset_index()\n",
    "        .pivot_table(index=\"AGENT COLOR\", columns=\"OUTCOME\", values=\"GAMES\")\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    viz_df.plot(kind=\"barh\", stacked=True)\n",
    "\n",
    "    plt.xlabel(\"Percentage\")\n",
    "    plt.title(f\"EVALUATION RESULTS FOR {n_games} GAMES\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2322b313ed754490a877fcb6a1b5ee24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = AGENT_EVALUATION(Stockfish_path, WHITE_PLAYER_POLICY, BLACK_PLAYER_POLICY, n_evaluations=5) #changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGENT COLOR</th>\n",
       "      <th>OUTCOME</th>\n",
       "      <th>N STEPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHITE</td>\n",
       "      <td>LOSS</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLACK</td>\n",
       "      <td>LOSS</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHITE</td>\n",
       "      <td>LOSS</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLACK</td>\n",
       "      <td>LOSS</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WHITE</td>\n",
       "      <td>LOSS</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AGENT COLOR OUTCOME  N STEPS\n",
       "0       WHITE    LOSS       28\n",
       "1       BLACK    LOSS        7\n",
       "2       WHITE    LOSS       11\n",
       "3       BLACK    LOSS       27\n",
       "4       WHITE    LOSS       19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready to Play Chess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state):\n",
    "    return np.c_[state[:,:,:14], state[:,:,-7:]] #the state we want just has the current board and the last matrixes with information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 21)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state= env.reset()\n",
    "state_p=preprocess_state(state)\n",
    "state_p.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create environment\n",
    "env= gym.make(\"ChessAlphaZero-v0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Deep Q-Network (DQN) model architecture using a neural network framework like TensorFlow. The model takes the state as input and outputs Q-values for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_WHITE:\n",
    "    def __init__(self, state_size, env):\n",
    "        #define environment\n",
    "        self.env= env\n",
    "\n",
    "        #define the state size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        #define the action size\n",
    "        self.action_size = env.observation_space.shape\n",
    "\n",
    "        #define legal actions\n",
    "        self.legal_actions = env.legal_actions\n",
    "        \n",
    "        #define the replay buffer\n",
    "        self.replay_buffer = deque(maxlen=1000)\n",
    "        \n",
    "        #define the discount factor\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        #define the epsilon value\n",
    "        self.epsilon = 0.99\n",
    "        \n",
    "        #define the update rate at which we want to update the target network\n",
    "        self.update_rate = 5\n",
    "        \n",
    "        #define the main network\n",
    "        self.main_network = self.build_network()\n",
    "        \n",
    "        #define the target network\n",
    "        self.target_network = self.build_network()\n",
    "        \n",
    "        #copy the weights of the main network to the target network\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "\n",
    "        #learning rate\n",
    "        self.learning_rate = .0001\n",
    "        \n",
    "\n",
    "    #Let's define a function called build_network which is essentially our DQN. \n",
    "\n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(1,states)))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(actions, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=.0001, epsilon=1e-7))\n",
    "\n",
    "        return model\n",
    "\n",
    "    #We learned that we train DQN by randomly sampling a minibatch of transitions from the\n",
    "    #replay buffer. So, we define a function called store_transition which stores the transition information\n",
    "    #into the replay buffer\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "\n",
    "    #We learned that in DQN, to take care of exploration-exploitation trade off, we select action\n",
    "    #using the epsilon-greedy policy. So, now we define the function called epsilon_greedy\n",
    "    #for selecting action using the epsilon-greedy policy.\n",
    "    def epsilon_greedy(self, state):\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            print('random')\n",
    "            legal_actions = self.env.legal_actions\n",
    "            action = np.random.choice(legal_actions)\n",
    "            return action\n",
    "        print('predict')\n",
    "        Q_values = self.main_network.predict(state, verbose=0)\n",
    "        print(Q_values[0])\n",
    "        return np.argmax(Q_values[0])\n",
    "        \"\"\"max_q_value = float('-inf') #since we want to maximize, we initialize in a very low value\n",
    "        selected_action = None\n",
    "\n",
    "        for action, q_value in zip(env.actions, Q_values[0]):\n",
    "            move = env.decode(action)  # Decode the action if needed\n",
    "            if env.encode(move) in self.legal_actions:\n",
    "                if q_value > max_q_value:\n",
    "                    max_q_value = q_value\n",
    "                    selected_action = move\n",
    "        return selected_action\"\"\"\n",
    "    \n",
    "    #train the network\n",
    "    def train(self, batch_size):\n",
    "        \n",
    "        minibatch = np.array(random.sample(self.replay_buffer, batch_size), dtype=object)\n",
    "\n",
    "        state_list = np.array(minibatch[:,0], dtype=object)\n",
    "        state_list = np.hstack(state_list).reshape(batch_size, 8, 8, 21)\n",
    "\n",
    "        next_state_list = np.array(minibatch[:,3])\n",
    "        next_state_list = np.hstack(next_state_list).reshape(batch_size, 8, 8, 21)\n",
    "\n",
    "        current_Q_values_list = self.main_network.predict(state_list, verbose=0)\n",
    "\n",
    "        max_q = np.amax(self.target_network.predict(next_state_list, verbose=0), axis=1)\n",
    "\n",
    "        for i, zip_ in enumerate(minibatch):\n",
    "\n",
    "            state, action, reward, next_state, done = zip_\n",
    "\n",
    "            if not done:\n",
    "                target  = reward + self.gamma * max_q[i]\n",
    "            else:\n",
    "                target = reward\n",
    "\n",
    "            updated_Q_value = target # (1 - self.learning_rate)*current_Q_values_list[i][action] + self.learning_rate*(target) # - current_Q_values_list[i][action]) # This is a different form of Q-learning (Min Q-Learning)\n",
    "\n",
    "            current_Q_values_list[i][action] = updated_Q_value\n",
    "        \n",
    "\n",
    "        #train the main network\n",
    "        self.main_network.fit(state_list, current_Q_values_list, epochs=1, verbose=0)\n",
    "            \n",
    "    #update the target network weights by copying from the main network\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_BLACK:\n",
    "    def __init__(self, state_size, env):\n",
    "        #define environment\n",
    "        self.env= env\n",
    "\n",
    "        #define the state size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        #define the action size\n",
    "        self.action_size = env.observation_space.shape\n",
    "\n",
    "        #define legal actions\n",
    "        self.legal_actions = env.legal_actions\n",
    "        \n",
    "        #define the replay buffer\n",
    "        self.replay_buffer = deque(maxlen=1000)\n",
    "        \n",
    "        #define the discount factor\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        #define the epsilon value\n",
    "        self.epsilon = 0.99\n",
    "        \n",
    "        #define the update rate at which we want to update the target network\n",
    "        self.update_rate = 5\n",
    "        \n",
    "        #define the main network\n",
    "        self.main_network = self.build_network()\n",
    "        \n",
    "        #define the target network\n",
    "        self.target_network = self.build_network()\n",
    "        \n",
    "        #copy the weights of the main network to the target network\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "\n",
    "        #learning rate\n",
    "        self.learning_rate = .0001\n",
    "        \n",
    "\n",
    "    #Let's define a function called build_network which is essentially our DQN. \n",
    "\n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(1,states)))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(actions, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=.0001, epsilon=1e-7))\n",
    "\n",
    "        return model\n",
    "\n",
    "    #We learned that we train DQN by randomly sampling a minibatch of transitions from the\n",
    "    #replay buffer. So, we define a function called store_transition which stores the transition information\n",
    "    #into the replay buffer\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "\n",
    "    #We learned that in DQN, to take care of exploration-exploitation trade off, we select action\n",
    "    #using the epsilon-greedy policy. So, now we define the function called epsilon_greedy\n",
    "    #for selecting action using the epsilon-greedy policy.\n",
    "    def epsilon_greedy(self, state):\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            print('random')\n",
    "            legal_actions = self.env.legal_actions\n",
    "            action = np.random.choice(legal_actions)\n",
    "            return action\n",
    "        print('predict')\n",
    "        Q_values = self.main_network.predict(state, verbose=0)\n",
    "        print(Q_values[0])\n",
    "        return np.argmax(Q_values[0])\n",
    "        \"\"\"max_q_value = float('-inf') #since we want to maximize, we initialize in a very low value\n",
    "        selected_action = None\n",
    "\n",
    "        for action, q_value in zip(env.actions, Q_values[0]):\n",
    "            move = env.decode(action)  # Decode the action if needed\n",
    "            if env.encode(move) in self.legal_actions:\n",
    "                if q_value > max_q_value:\n",
    "                    max_q_value = q_value\n",
    "                    selected_action = move\n",
    "        return selected_action\"\"\"\n",
    "    \n",
    "    #train the network\n",
    "    def train(self, batch_size):\n",
    "        \n",
    "        minibatch = np.array(random.sample(self.replay_buffer, batch_size), dtype=object)\n",
    "\n",
    "        state_list = np.array(minibatch[:,0], dtype=object)\n",
    "        state_list = np.hstack(state_list).reshape(batch_size, 8, 8, 21)\n",
    "\n",
    "        next_state_list = np.array(minibatch[:,3])\n",
    "        next_state_list = np.hstack(next_state_list).reshape(batch_size, 8, 8, 21)\n",
    "\n",
    "        current_Q_values_list = self.main_network.predict(state_list, verbose=0)\n",
    "\n",
    "        max_q = np.amax(self.target_network.predict(next_state_list, verbose=0), axis=1)\n",
    "\n",
    "        for i, zip_ in enumerate(minibatch):\n",
    "\n",
    "            state, action, reward, next_state, done = zip_\n",
    "\n",
    "            if not done:\n",
    "                target  = reward + self.gamma * max_q[i]\n",
    "            else:\n",
    "                target = reward\n",
    "\n",
    "            updated_Q_value = target # (1 - self.learning_rate)*current_Q_values_list[i][action] + self.learning_rate*(target) # - current_Q_values_list[i][action]) # This is a different form of Q-learning (Min Q-Learning)\n",
    "\n",
    "            current_Q_values_list[i][action] = updated_Q_value\n",
    "        \n",
    "\n",
    "        #train the main network\n",
    "        self.main_network.fit(state_list, current_Q_values_list, epochs=1, verbose=0)\n",
    "            \n",
    "    #update the target network weights by copying from the main network\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying BoltzmannQPolicy\n",
    "def WHITE_BoltzmannQPolicy(env, state):\n",
    "    policy= BoltzmannQPolicy(temperature=1.0)\n",
    "    q_values = model_white.predict(state)\n",
    "    return policy.select_action(q_values)\n",
    "\n",
    "def BLACK_BoltzmannQPolicy(env, state):\n",
    "    policy= BoltzmannQPolicy(temperature=1.0)\n",
    "    q_values = model_white.predict(state)\n",
    "    return policy.select_action(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_WHITE_scenario(Stockfish_path, dqn_white, evaluation_number):\n",
    "    env = gym.make(\"ChessAlphaZero-v0\") # We will use Alpha Zero's numenclature for the actions encodings\n",
    "    stockfish = Stockfish(Stockfish_path)\n",
    "    stockfish.set_elo_rating(\n",
    "        100\n",
    "    )  # Default \"skill\" level is 1350, higher will increase the skill of stockfish \"player\". See more at https://en.wikipedia.org/wiki/Elo_rating_system\n",
    "    \n",
    "    #pre-process state\n",
    "    state = env.reset()\n",
    "    state = preprocess_state(state)\n",
    "    done = False\n",
    "    counter = 0  # Since each step represents a play in a chess game we are going to store the number of steps associated to the episode/game\n",
    "    # set return to 0\n",
    "    Return = 0 \n",
    "    Real_Return = 0\n",
    "\n",
    "    while not done:\n",
    "        if (\n",
    "            counter % 2 == 0\n",
    "        ):  # If the step number is even, this means that it is the WHITE player's turn\n",
    "            \n",
    "            #Update target model if the correct nÂº episodes has passed\n",
    "            if evaluation_number % dqn_white.update_rate == 0:\n",
    "                dqn_white.update_target_network()\n",
    "\n",
    "            # Select action to perform   \n",
    "            action = dqn_white.epsilon_greedy(state)\n",
    "            decoded_action = str(env.decode(action))\n",
    "            print('decoded action number: ', decoded_action)\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "\n",
    "            # Perform selected action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            real_reward= reward\n",
    "\n",
    "            # Reward full trotle?\n",
    "            #if env.decode(action)[1]>0:\n",
    "            #    reward *= 1.5\n",
    "\n",
    "            #pre-process next state\n",
    "            next_state = preprocess_state(next_state)\n",
    "            \n",
    "            #update values in dqn class\n",
    "            dqn_white.env= env\n",
    "            dqn_white.legal_actions = env.legal_actions\n",
    "\n",
    "            #store the transition information\n",
    "            dqn_white.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "            #update current state to next state\n",
    "            state = next_state\n",
    "\n",
    "            #update the return\n",
    "            Return += reward\n",
    "            Real_Return += real_reward\n",
    "\n",
    "            #TODO Step: if continually getting negative reward, stop\n",
    "\n",
    "            #print return if episode finished\n",
    "            if done:\n",
    "                print('Episode: ',i, ', Return:', round(Return), 'Steps:', counter, 'Epsilon:', round(dqn_white.epsilon,2), '\\n')\n",
    "                break\n",
    "\n",
    "            #if nÂº transitions in replay_buffer>batch_size\n",
    "            if (len(dqn_white.replay_buffer) > batch_size) & (counter/2 % 10 == 0): # Only train each 10 steps that the agent plays\n",
    "                dqn_white.train(batch_size)\n",
    "\n",
    "\n",
    "        else:  # If the step number is not even, aka odd, this means that it is the BLACK player's turn\n",
    "            decoded_action = stockfish.get_best_move()\n",
    "            action = env.encode(chess.Move.from_uci(decoded_action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return reward, np.ceil(counter / 2)\n",
    "\n",
    "\n",
    "def generate_BLACK_scenario(Stockfish_path, dqn_black, evaluation_number):\n",
    "    env = gym.make(\"ChessAlphaZero-v0\") # We will use Alpha Zero's numenclature for the actions encodings\n",
    "    stockfish = Stockfish(Stockfish_path)\n",
    "    stockfish.set_elo_rating(\n",
    "        100\n",
    "    )  # Default \"skill\" level is 1350, higher will increase the skill of stockfish \"player\". See more at https://en.wikipedia.org/wiki/Elo_rating_system\n",
    "    \n",
    "    #pre-process state\n",
    "    state = env.reset()\n",
    "    state = preprocess_state(state)\n",
    "    done = False\n",
    "    counter = 0  # Since each step represents a play in a chess game we are going to store the number of steps associated to the episode/game\n",
    "    # set return to 0\n",
    "    Return = 0 \n",
    "    Real_Return = 0\n",
    "\n",
    "    while not done:\n",
    "        if (\n",
    "            counter % 2 == 1\n",
    "        ):  # If the step number is even, this means that it is the WHITE player's turn\n",
    "            \n",
    "            #Update target model if the correct nÂº episodes has passed\n",
    "            if evaluation_number % dqn_black.update_rate == 0:\n",
    "                dqn_black.update_target_network()\n",
    "\n",
    "            # Select action to perform   \n",
    "            action = dqn_black.epsilon_greedy(state)\n",
    "            decoded_action = str(env.decode(action))\n",
    "            print('decoded action number: ', decoded_action)\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "\n",
    "            # Perform selected action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            real_reward= reward\n",
    "\n",
    "            # Reward full trotle?\n",
    "            #if env.decode(action)[1]>0:\n",
    "            #    reward *= 1.5\n",
    "\n",
    "            #pre-process next state\n",
    "            next_state = preprocess_state(next_state)\n",
    "            \n",
    "            dqn_black.env= env\n",
    "            dqn_black.legal_actions = env.legal_actions\n",
    "            \n",
    "            #store the transition information\n",
    "            dqn_black.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "            #update current state to next state\n",
    "            state = next_state\n",
    "\n",
    "            #update the return\n",
    "            Return += reward\n",
    "            Real_Return += real_reward\n",
    "\n",
    "            #TODO Step: if continually getting negative reward, stop\n",
    "\n",
    "            #print return if episode finished\n",
    "            if done:\n",
    "                print('Episode: ',i, ', Return:', round(Return), 'Steps:', counter, 'Epsilon:', round(dqn_black.epsilon,2), '\\n')\n",
    "                break\n",
    "\n",
    "            #if nÂº transitions in replay_buffer>batch_size\n",
    "            if (len(dqn_black.replay_buffer) > batch_size) & (counter/2 % 10 == 0): # Only train each 10 steps that the agent plays\n",
    "                dqn_black.train(batch_size)\n",
    "\n",
    "\n",
    "        else:  # If the step number is not even, aka odd, this means that it is the BLACK player's turn\n",
    "            decoded_action = stockfish.get_best_move()\n",
    "            action = env.encode(chess.Move.from_uci(decoded_action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return reward, np.ceil(counter / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AGENT_EVALUATION(Stockfish_path, n_evaluations=100): #changed\n",
    "    results_list = []\n",
    "    env= gym.make(\"ChessAlphaZero-v0\")\n",
    "    env.reset()\n",
    "    dqn_white= DQN_WHITE((8, 8, 21), env)\n",
    "    dqn_black= DQN_BLACK((8, 8, 21), env)\n",
    "\n",
    "    for evaluation_number in tqdm(range(n_evaluations)):\n",
    "        \n",
    "        generate_episode = generate_WHITE_scenario\n",
    "        reward, n_steps = generate_episode(Stockfish_path, dqn_white, evaluation_number) #changed\n",
    "\n",
    "        if reward == 1:\n",
    "            result = \"VICTORY\"\n",
    "        elif reward == 0:\n",
    "            result = \"DRAW\"\n",
    "        else:\n",
    "            result = \"LOSS\"\n",
    "\n",
    "        results_list.append([\"WHITE\", result, n_steps])\n",
    "        \n",
    "        #update the epsilon\n",
    "        dqn_white.epsilon -= .005 # dqn.epsilon/num_episodes\n",
    "        dqn_white.epsilon = max(dqn_white.epsilon, 0.2) \n",
    "\n",
    "        print('in black scenario')\n",
    "        generate_episode = generate_BLACK_scenario\n",
    "\n",
    "        reward, n_steps = generate_episode(Stockfish_path, dqn_black, evaluation_number) #changed\n",
    "\n",
    "        if reward == -1:\n",
    "            result = \"VICTORY\"\n",
    "        elif reward == 0:\n",
    "            result = \"DRAW\"\n",
    "        else:\n",
    "            result = \"LOSS\"\n",
    "\n",
    "        results_list.append([\"BLACK\", result, n_steps])\n",
    "\n",
    "        #update the epsilon\n",
    "        dqn_black.epsilon -= .005 # dqn.epsilon/num_episodes\n",
    "        dqn_black.epsilon = max(dqn_black.epsilon, 0.2) \n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        results_list, columns=[\"AGENT COLOR\", \"OUTCOME\", \"N STEPS\"]\n",
    "    ).astype(\"int\", errors=\"ignore\")\n",
    "\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "    results_group = (\n",
    "        df.groupby([\"AGENT COLOR\", \"OUTCOME\"])\n",
    "        .count()\n",
    "        .rename(columns={\"N STEPS\": \"GAMES\"})\n",
    "    )\n",
    "\n",
    "    n_games = results_group.sum()[0]\n",
    "\n",
    "    results_group = (2 * 100 * results_group / (n_games)).astype(\"int\")\n",
    "\n",
    "    viz_df = (\n",
    "        results_group.reset_index()\n",
    "        .pivot_table(index=\"AGENT COLOR\", columns=\"OUTCOME\", values=\"GAMES\")\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    viz_df.plot(kind=\"barh\", stacked=True)\n",
    "\n",
    "    plt.xlabel(\"Percentage\")\n",
    "    plt.title(f\"EVALUATION RESULTS FOR {n_games} GAMES\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebe59893dcd4b928f17e62deabf47f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random\n",
      "decoded action number:  b1c3\n",
      "random\n",
      "decoded action number:  c3e4\n",
      "random\n",
      "decoded action number:  b2b4\n",
      "random\n",
      "decoded action number:  h2h4\n",
      "random\n",
      "decoded action number:  h1h2\n",
      "random\n",
      "decoded action number:  a1b1\n",
      "random\n",
      "decoded action number:  h4h5\n",
      "random\n",
      "decoded action number:  a2a4\n",
      "random\n",
      "decoded action number:  c2c3\n",
      "random\n",
      "decoded action number:  a4a5\n",
      "random\n",
      "decoded action number:  d1c2\n",
      "random\n",
      "decoded action number:  b1b2\n",
      "random\n",
      "decoded action number:  a5a6\n",
      "random\n",
      "decoded action number:  c2c6\n",
      "random\n",
      "decoded action number:  c6c3\n",
      "random\n",
      "decoded action number:  c3b3\n",
      "random\n",
      "decoded action number:  a6a7\n",
      "in black scenario\n",
      "random\n",
      "decoded action number:  b7b5\n",
      "random\n",
      "decoded action number:  h7h6\n",
      "random\n",
      "decoded action number:  d7d5\n",
      "random\n",
      "decoded action number:  a7a5\n",
      "random\n",
      "decoded action number:  f7f6\n",
      "predict\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected flatten_144_input to have shape (1, 8) but got array with shape (8, 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[250], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Define the batch size:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m \u001b[39m#128\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df \u001b[39m=\u001b[39m AGENT_EVALUATION(Stockfish_path, n_evaluations\u001b[39m=\u001b[39;49mnum_episodes) \u001b[39m#changed\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[248], line 29\u001b[0m, in \u001b[0;36mAGENT_EVALUATION\u001b[1;34m(Stockfish_path, n_evaluations)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39min black scenario\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m generate_episode \u001b[39m=\u001b[39m generate_BLACK_scenario\n\u001b[1;32m---> 29\u001b[0m reward, n_steps \u001b[39m=\u001b[39m generate_episode(Stockfish_path, dqn_black, evaluation_number) \u001b[39m#changed\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m reward \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m     32\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mVICTORY\u001b[39m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[247], line 108\u001b[0m, in \u001b[0;36mgenerate_BLACK_scenario\u001b[1;34m(Stockfish_path, dqn_black, evaluation_number)\u001b[0m\n\u001b[0;32m    105\u001b[0m     dqn_black\u001b[39m.\u001b[39mupdate_target_network()\n\u001b[0;32m    107\u001b[0m \u001b[39m# Select action to perform   \u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m action \u001b[39m=\u001b[39m dqn_black\u001b[39m.\u001b[39;49mepsilon_greedy(state)\n\u001b[0;32m    109\u001b[0m decoded_action \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(env\u001b[39m.\u001b[39mdecode(action))\n\u001b[0;32m    110\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdecoded action number: \u001b[39m\u001b[39m'\u001b[39m, decoded_action)\n",
      "Cell \u001b[1;32mIn[244], line 71\u001b[0m, in \u001b[0;36mDQN_BLACK.epsilon_greedy\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m action\n\u001b[0;32m     70\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m Q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain_network\u001b[39m.\u001b[39;49mpredict(state, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     72\u001b[0m \u001b[39mprint\u001b[39m(Q_values[\u001b[39m0\u001b[39m])\n\u001b[0;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39margmax(Q_values[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\keras\\engine\\training_v1.py:1059\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m\"\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1058\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m-> 1059\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mpredict(\n\u001b[0;32m   1060\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1061\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1062\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1063\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1064\u001b[0m     steps\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   1065\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1066\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1067\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1068\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1069\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:798\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.predict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m    788\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    789\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    796\u001b[0m ):\n\u001b[0;32m    797\u001b[0m     batch_size \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[1;32m--> 798\u001b[0m     x, _, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49m_standardize_user_data(\n\u001b[0;32m    799\u001b[0m         x, check_steps\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, steps_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps\u001b[39;49m\u001b[39m\"\u001b[39;49m, steps\u001b[39m=\u001b[39;49msteps\n\u001b[0;32m    800\u001b[0m     )\n\u001b[0;32m    801\u001b[0m     \u001b[39mreturn\u001b[39;00m predict_loop(\n\u001b[0;32m    802\u001b[0m         model,\n\u001b[0;32m    803\u001b[0m         x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    808\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\keras\\engine\\training_v1.py:2652\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2643\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2644\u001b[0m     \u001b[39mnot\u001b[39;00m run_eagerly\n\u001b[0;32m   2645\u001b[0m     \u001b[39mand\u001b[39;00m is_build_called\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2648\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(_is_symbolic_tensor(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m all_inputs)\n\u001b[0;32m   2649\u001b[0m ):\n\u001b[0;32m   2650\u001b[0m     \u001b[39mreturn\u001b[39;00m [], [], \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_standardize_tensors(\n\u001b[0;32m   2653\u001b[0m     x,\n\u001b[0;32m   2654\u001b[0m     y,\n\u001b[0;32m   2655\u001b[0m     sample_weight,\n\u001b[0;32m   2656\u001b[0m     run_eagerly\u001b[39m=\u001b[39;49mrun_eagerly,\n\u001b[0;32m   2657\u001b[0m     dict_inputs\u001b[39m=\u001b[39;49mdict_inputs,\n\u001b[0;32m   2658\u001b[0m     is_dataset\u001b[39m=\u001b[39;49mis_dataset,\n\u001b[0;32m   2659\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2660\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2661\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\keras\\engine\\training_v1.py:2693\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2690\u001b[0m \u001b[39m# Standardize the inputs.\u001b[39;00m\n\u001b[0;32m   2691\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset)):\n\u001b[0;32m   2692\u001b[0m     \u001b[39m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[1;32m-> 2693\u001b[0m     x \u001b[39m=\u001b[39m training_utils_v1\u001b[39m.\u001b[39;49mstandardize_input_data(\n\u001b[0;32m   2694\u001b[0m         x,\n\u001b[0;32m   2695\u001b[0m         feed_input_names,\n\u001b[0;32m   2696\u001b[0m         feed_input_shapes,\n\u001b[0;32m   2697\u001b[0m         check_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[0;32m   2698\u001b[0m         exception_prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2699\u001b[0m     )\n\u001b[0;32m   2701\u001b[0m \u001b[39m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[0;32m   2702\u001b[0m \u001b[39m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[0;32m   2703\u001b[0m \u001b[39m# at all times - validate that this is so and refactor the\u001b[39;00m\n\u001b[0;32m   2704\u001b[0m \u001b[39m# standardization code.\u001b[39;00m\n\u001b[0;32m   2705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n",
      "File \u001b[1;32mc:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:731\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[39mfor\u001b[39;00m dim, ref_dim \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(data_shape, shape):\n\u001b[0;32m    726\u001b[0m                 \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    727\u001b[0m                     ref_dim \u001b[39m!=\u001b[39m dim\n\u001b[0;32m    728\u001b[0m                     \u001b[39mand\u001b[39;00m ref_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    729\u001b[0m                     \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    730\u001b[0m                 ):\n\u001b[1;32m--> 731\u001b[0m                     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    732\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mError when checking \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    733\u001b[0m                         \u001b[39m+\u001b[39m exception_prefix\n\u001b[0;32m    734\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m: expected \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    735\u001b[0m                         \u001b[39m+\u001b[39m names[i]\n\u001b[0;32m    736\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m to have shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(shape)\n\u001b[0;32m    738\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m but got array with shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(data_shape)\n\u001b[0;32m    740\u001b[0m                     )\n\u001b[0;32m    741\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected flatten_144_input to have shape (1, 8) but got array with shape (8, 21)"
     ]
    }
   ],
   "source": [
    "#number of episodes\n",
    "num_episodes = 20#00\n",
    "\n",
    "# Define the batch size:\n",
    "batch_size = 32 #128\n",
    "\n",
    "df = AGENT_EVALUATION(Stockfish_path, n_evaluations=num_episodes) #changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.pgn\n",
    "import io\n",
    "\n",
    "pgn = open(\"C:/Users/isabe/Desktop/RL/Project/alphazero_stockfish_all/alphazero_vs_stockfish_all.pgn\")\n",
    "\n",
    "train_df=[]\n",
    "\n",
    "for i in range(110):\n",
    "    game=chess.pgn.read_game(pgn)\n",
    "    # Process the moves, positions in board, rewards and outcome in the game\n",
    "    moves = []\n",
    "    positions = []\n",
    "    rewards = []\n",
    "    outcome = game.headers[\"Result\"]\n",
    "    if game.headers[\"White\"]== \"AlphaZero\":\n",
    "        color= \"white\"\n",
    "    else:\n",
    "        color= \"black\"\n",
    "\n",
    "    board = game.board()\n",
    "    for move in game.mainline_moves():\n",
    "        board.push(move)\n",
    "        moves.append(move)\n",
    "        positions.append(board.fen())  # Save board position\n",
    "        \n",
    "    game_data = {\n",
    "        \"moves\": moves,\n",
    "        \"positions\": positions,\n",
    "        \"player_color\": color,\n",
    "        \"outcome\": outcome\n",
    "    }\n",
    "\n",
    "    train_df.append(game_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moves</th>\n",
       "      <th>positions</th>\n",
       "      <th>player_color</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[g1f3, g8f6, c2c4, e7e6, b1c3, f8b4, d1c2, e8g...</td>\n",
       "      <td>[rnbqkbnr/pppppppp/8/8/8/5N2/PPPPPPPP/RNBQKB1R...</td>\n",
       "      <td>white</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[d2d4, g8f6, c2c4, e7e6, g1f3, b7b6, g2g3, c8b...</td>\n",
       "      <td>[rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR...</td>\n",
       "      <td>white</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[d2d4, g8f6, c2c4, e7e6, g1f3, b7b6, g2g3, c8b...</td>\n",
       "      <td>[rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR...</td>\n",
       "      <td>white</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[g1f3, e7e6, c2c4, g8f6, b1c3, f8b4, d1c2, e8g...</td>\n",
       "      <td>[rnbqkbnr/pppppppp/8/8/8/5N2/PPPPPPPP/RNBQKB1R...</td>\n",
       "      <td>white</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[d2d4, g8f6, g1f3, e7e6, c2c4, b7b6, g2g3, c8b...</td>\n",
       "      <td>[rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR...</td>\n",
       "      <td>white</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               moves  \\\n",
       "0  [g1f3, g8f6, c2c4, e7e6, b1c3, f8b4, d1c2, e8g...   \n",
       "1  [d2d4, g8f6, c2c4, e7e6, g1f3, b7b6, g2g3, c8b...   \n",
       "2  [d2d4, g8f6, c2c4, e7e6, g1f3, b7b6, g2g3, c8b...   \n",
       "3  [g1f3, e7e6, c2c4, g8f6, b1c3, f8b4, d1c2, e8g...   \n",
       "4  [d2d4, g8f6, g1f3, e7e6, c2c4, b7b6, g2g3, c8b...   \n",
       "\n",
       "                                           positions player_color outcome  \n",
       "0  [rnbqkbnr/pppppppp/8/8/8/5N2/PPPPPPPP/RNBQKB1R...        white     1-0  \n",
       "1  [rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR...        white     1-0  \n",
       "2  [rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR...        white     1-0  \n",
       "3  [rnbqkbnr/pppppppp/8/8/8/5N2/PPPPPPPP/RNBQKB1R...        white     1-0  \n",
       "4  [rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR...        white     1-0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentativa YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.model import Sequential()\n",
    "from keras.layers import Dense, Dropout, COnv2D, MaxPooling, Activation, Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000020062636710> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x0000020062636710>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: lineno is out of bounds\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000020062636710> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x0000020062636710>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: lineno is out of bounds\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 95ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/20000 [00:00<06:08, 54.26episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000020062C8C160> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x0000020062C8C160>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: lineno is out of bounds\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000020062C8C160> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x0000020062C8C160>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: lineno is out of bounds\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000020062637F40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000020062637F40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/20000 [00:03<1:16:00,  4.38episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\isabe\\AppData\\Local\\Temp\\ipykernel_14916\\746978122.py\", line 379, in <module>\n",
      "    agent.train(done, step)\n",
      "  File \"C:\\Users\\isabe\\AppData\\Local\\Temp\\ipykernel_14916\\746978122.py\", line 326, in train\n",
      "    self.model.fit(np.array(X)/255, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\keras\\callbacks.py\", line 2557, in _train_writer\n",
      "AttributeError: 'ModifiedTensorBoard' object has no attribute '_train_dir'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\isabe\\anaconda3\\envs\\week5\\lib\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.backend import set_session \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
    "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
    "MODEL_NAME = '2x256'\n",
    "MIN_REWARD = -200  # For model save\n",
    "MEMORY_FRACTION = 0.20\n",
    "\n",
    "# Environment settings\n",
    "EPISODES = 20_000\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 50  # episodes\n",
    "SHOW_PREVIEW = False\n",
    "\n",
    "\n",
    "class Blob:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.randint(0, size)\n",
    "        self.y = np.random.randint(0, size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Blob ({self.x}, {self.y})\"\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def action(self, choice):\n",
    "        '''\n",
    "        Gives us 9 total movement options. (0,1,2,3,4,5,6,7,8)\n",
    "        '''\n",
    "        if choice == 0:\n",
    "            self.move(x=1, y=1)\n",
    "        elif choice == 1:\n",
    "            self.move(x=-1, y=-1)\n",
    "        elif choice == 2:\n",
    "            self.move(x=-1, y=1)\n",
    "        elif choice == 3:\n",
    "            self.move(x=1, y=-1)\n",
    "\n",
    "        elif choice == 4:\n",
    "            self.move(x=1, y=0)\n",
    "        elif choice == 5:\n",
    "            self.move(x=-1, y=0)\n",
    "\n",
    "        elif choice == 6:\n",
    "            self.move(x=0, y=1)\n",
    "        elif choice == 7:\n",
    "            self.move(x=0, y=-1)\n",
    "\n",
    "        elif choice == 8:\n",
    "            self.move(x=0, y=0)\n",
    "\n",
    "    def move(self, x=False, y=False):\n",
    "\n",
    "        # If no value for x, move randomly\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "\n",
    "        # If no value for y, move randomly\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "        # If we are out of bounds, fix!\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > self.size-1:\n",
    "            self.x = self.size-1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > self.size-1:\n",
    "            self.y = self.size-1\n",
    "\n",
    "\n",
    "class BlobEnv:\n",
    "    SIZE = 10\n",
    "    RETURN_IMAGES = True\n",
    "    MOVE_PENALTY = 1\n",
    "    ENEMY_PENALTY = 300\n",
    "    FOOD_REWARD = 25\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)  # 4\n",
    "    ACTION_SPACE_SIZE = 9\n",
    "    PLAYER_N = 1  # player key in dict\n",
    "    FOOD_N = 2  # food key in dict\n",
    "    ENEMY_N = 3  # enemy key in dict\n",
    "    # the dict! (colors)\n",
    "    d = {1: (255, 175, 0),\n",
    "         2: (0, 255, 0),\n",
    "         3: (0, 0, 255)}\n",
    "\n",
    "    def reset(self):\n",
    "        self.player = Blob(self.SIZE)\n",
    "        self.food = Blob(self.SIZE)\n",
    "        while self.food == self.player:\n",
    "            self.food = Blob(self.SIZE)\n",
    "        self.enemy = Blob(self.SIZE)\n",
    "        while self.enemy == self.player or self.enemy == self.food:\n",
    "            self.enemy = Blob(self.SIZE)\n",
    "\n",
    "        self.episode_step = 0\n",
    "\n",
    "        if self.RETURN_IMAGES:\n",
    "            observation = np.array(self.get_image())\n",
    "        else:\n",
    "            observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        self.player.action(action)\n",
    "\n",
    "        #### MAYBE ###\n",
    "        #enemy.move()\n",
    "        #food.move()\n",
    "        ##############\n",
    "\n",
    "        if self.RETURN_IMAGES:\n",
    "            new_observation = np.array(self.get_image())\n",
    "        else:\n",
    "            new_observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "\n",
    "        if self.player == self.enemy:\n",
    "            reward = -self.ENEMY_PENALTY\n",
    "        elif self.player == self.food:\n",
    "            reward = self.FOOD_REWARD\n",
    "        else:\n",
    "            reward = -self.MOVE_PENALTY\n",
    "\n",
    "        done = False\n",
    "        if reward == self.FOOD_REWARD or reward == -self.ENEMY_PENALTY or self.episode_step >= 200:\n",
    "            done = True\n",
    "\n",
    "        return new_observation, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((300, 300))  # resizing so we can see our agent in all its glory.\n",
    "        cv2.imshow(\"image\", np.array(img))  # show it!\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    # FOR CNN #\n",
    "    def get_image(self):\n",
    "        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)  # starts an rbg of our size\n",
    "        env[self.food.x][self.food.y] = self.d[self.FOOD_N]  # sets the food location tile to green color\n",
    "        env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]  # sets the enemy location to red\n",
    "        env[self.player.x][self.player.y] = self.d[self.PLAYER_N]  # sets the player tile to blue\n",
    "        img = Image.fromarray(env, 'RGB')  # reading to rgb. Apparently. Even tho color definitions are bgr. ???\n",
    "        return img\n",
    "\n",
    "\n",
    "env = BlobEnv()\n",
    "\n",
    "# For stats\n",
    "ep_rewards = [-200]\n",
    "\n",
    "# For more repetitive results\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# Memory fraction, used mostly when trai8ning multiple agents\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION)\n",
    "#backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)))\n",
    "\n",
    "# Create models folder\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "\n",
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer =tf.summary.create_file_writer(self.log_dir) #tf.summary.FileWriter(self.log_dir)\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)\n",
    "\n",
    "    def _write_logs(self, logs, index):\n",
    "        with self.writer.as_default():\n",
    "            for name, value in logs.items():\n",
    "                tf.summary.scalar(name, value, step=index)\n",
    "                self.step += 1\n",
    "                self.writer.flush()\n",
    "\n",
    "# Agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "\n",
    "        # Main model\n",
    "        self.model = self.create_model()\n",
    "\n",
    "        # Target network\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # An array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        # Custom tensorboard object\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(MODEL_NAME, int(time.time())))\n",
    "\n",
    "        # Used to count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES))  # OBSERVATION_SPACE_VALUES = (10, 10, 3) a 10x10 RGB image.\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "        model.add(Dense(64))\n",
    "\n",
    "        model.add(Dense(env.ACTION_SPACE_SIZE, activation='linear'))  # ACTION_SPACE_SIZE = how many choices (9)\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    # Adds step's data to a memory replay array\n",
    "    # (observation space, action, reward, new observation space, done)\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    # Trains main network every step during episode\n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X)/255, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
    "\n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "\n",
    "\n",
    "agent = DQNAgent()\n",
    "\n",
    "# Iterate over episodes\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "    # Update tensorboard step every episode\n",
    "    agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.reset()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
    "\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "            env.render()\n",
    "\n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "\n",
    "        # Save model, but only when min reward is greater or equal a set value\n",
    "        if min_reward >= MIN_REWARD:\n",
    "            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement keras.backend.tensorflow_backend (from versions: none)\n",
      "ERROR: No matching distribution found for keras.backend.tensorflow_backend\n"
     ]
    }
   ],
   "source": [
    "pip install keras.backend.tensorflow_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.backend import set_session "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
