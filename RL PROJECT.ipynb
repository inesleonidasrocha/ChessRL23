{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Final Project \n",
    "\n",
    "Welcome to your Reinforcement Learning project focused on developing an RL agent capable of playing chess at a strategic level. Chess has long been considered a benchmark for measuring AI capabilities, and this project aims to leverage the power of RL to create an intelligent agent that can make optimal decisions in complex chess positions. By combining the principles of reinforcement learning with the rich strategic domain of chess, you will explore new approaches to create the most effective chess player.\n",
    "\n",
    "## Project Objectives:\n",
    "\n",
    "* Train an RL agent to play chess: The primary objective of this project is to develop an RL agent that can play chess at a high level of proficiency. The agent should be capable of evaluating chess positions and making strategic decisions.\n",
    "\n",
    "* Optimize decision-making using RL algorithms: Explore different RL algorithms, as seen in class, to train the agent. Compare and analise their effectiveness in learning and decision-making capabilities in the context of chess.\n",
    "\n",
    "* Use a challenging chess environment: Use a comprehensive environment for the agent to interact with, representing the rules and dynamics of chess. This environment will provide a realistic and challenging setting for the agent's training and evaluation.\n",
    "\n",
    "* Evaluate and benchmark performance: Assess the performance of the RL agent against different benchmarks from existing chess engines. You will compare your agent's performance to established chess engines to measure progress and identify areas for improvement.\n",
    "\n",
    "\n",
    "### Extra Objectives:\n",
    "\n",
    "* Investigate transfer learning and generalization: Explore techniques for transfer learning to leverage knowledge acquired in related domains or from pre-training on large chess datasets. Investigate the agent's ability to generalize its knowledge.\n",
    "\n",
    "* Enhance interpretability and analysis: Develop methods to analise the agent's decision-making process and provide insights into its strategic thinking. Investigate techniques to visualize the agent's evaluation of chess positions and understand its reasoning behind specific moves."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Play Chess! \n",
    "\n",
    "As you know [Chess](https://en.wikipedia.org/wiki/Chess) is a board game for two players, called White and Black, each controlling an army of chess pieces in their color, with the objective to checkmate the opponent's king.\n",
    "\n",
    "Chess is an abstract strategy game that involves no hidden information and no use of dice or cards. It is played on a chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. White moves first, followed by Black. Checkmating the opponent's king involves putting the king under immediate attack (in \"check\") whereby there is no way for it to escape.\n",
    "\n",
    "\n",
    "![](Images/CHESS_MOVES.PNG)\n",
    "\n",
    "* The king moves one square in any direction. There is also a special move called castling that involves moving the king and a rook. The king is the most valuable piece — attacks on the king must be immediately countered, and if this is impossible, the game is immediately lost.\n",
    "* A rook can move any number of squares along a rank or file, but cannot leap over other pieces. Along with the king, a rook is involved during the king's castling move.\n",
    "* A bishop can move any number of squares diagonally, but cannot leap over other pieces.\n",
    "* A queen combines the power of a rook and bishop and can move any number of squares along a rank, file, or diagonal, but cannot leap over other pieces.\n",
    "* A knight moves to any of the closest squares that are not on the same rank, file, or diagonal. (Thus the move forms an \"L\"-shape: two squares vertically and one square horizontally, or two squares horizontally and one square vertically.) The knight is the only piece that can leap over other pieces.\n",
    "* A pawn can move forward to the unoccupied square immediately in front of it on the same file, or on its first move it can advance two squares along the same file, provided both squares are unoccupied (black dots in the diagram). A pawn can capture an opponent's piece on a square diagonally in front of it by moving to that square (black crosses). It cannot capture a piece while advancing along the same file. A pawn has two special moves: the en passant capture and promotion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The [Environment](https://github.com/iamlucaswolf/gym-chess)\n",
    "\n",
    "The environment gym-chess provides OpenAI Gym environments for the game of Chess. It comes with an implementation of the board and move encoding used in AlphaZero. \n",
    "\n",
    "Please install it using the command: \n",
    "\n",
    "`pip install gym-chess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stockfish import Stochfish\n",
    "#ver como fazer isto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gym-chess\n",
      "  Downloading gym_chess-0.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting gym<0.18.0,>=0.17.2 (from gym-chess)\n",
      "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-chess<0.32.0,>=0.31.1 (from gym-chess)\n",
      "  Downloading python_chess-0.31.4-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.6/134.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Library/Python/3.9/site-packages (from gym<0.18.0,>=0.17.2->gym-chess) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/nekas/Library/Python/3.9/lib/python/site-packages (from gym<0.18.0,>=0.17.2->gym-chess) (1.24.2)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0 (from gym<0.18.0,>=0.17.2->gym-chess)\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym<0.18.0,>=0.17.2->gym-chess)\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: future in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.18.0,>=0.17.2->gym-chess) (0.18.2)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654651 sha256=6b40cb5780a74a8047ba61578155cf2f0eee06f3ee450b80a88bc9c80f72170e\n",
      "  Stored in directory: /Users/nekas/Library/Caches/pip/wheels/5d/06/a4/57a926b2e87a5e1f21551577750549206dde639e19a5ac72d5\n",
      "Successfully built gym\n",
      "Installing collected packages: python-chess, pyglet, cloudpickle, gym, gym-chess\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.1\n",
      "    Uninstalling cloudpickle-2.2.1:\n",
      "      Successfully uninstalled cloudpickle-2.2.1\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.26.2\n",
      "    Uninstalling gym-0.26.2:\n",
      "      Successfully uninstalled gym-0.26.2\n",
      "Successfully installed cloudpickle-1.6.0 gym-0.17.3 gym-chess-0.1.1 pyglet-1.5.0 python-chess-0.31.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import gym\n",
    "import gym_chess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/nekas/Library/Python/3.9/lib/python/site-packages (1.24.2)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.3-cp39-cp39-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.2\n",
      "    Uninstalling numpy-1.24.2:\n",
      "      Successfully uninstalled numpy-1.24.2\n",
      "Successfully installed numpy-1.24.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Two player's game\n",
    "\n",
    "As you know chess is played by two players, as such the gym-chess environment gives you access to both players actions in a sequential matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WHITE_PLAYER_POLICY(env, state):\n",
    "    legal_actions = env.legal_actions\n",
    "    action = np.random.choice(legal_actions)\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def BLACK_PLAYER_POLICY(env, state):\n",
    "    legal_actions = env.legal_actions\n",
    "    action = np.random.choice(legal_actions)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mChessAlphaZero-v0\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m )  \u001b[39m# We will use Alpha Zero's numenclature for the actions encodings\u001b[39;00m\n\u001b[1;32m      5\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      6\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym/envs/registration.py:145\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake\u001b[39m(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mreturn\u001b[39;00m registry\u001b[39m.\u001b[39;49mmake(\u001b[39mid\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym/envs/registration.py:90\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mMaking new env: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, path)\n\u001b[1;32m     89\u001b[0m spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspec(path)\n\u001b[0;32m---> 90\u001b[0m env \u001b[39m=\u001b[39m spec\u001b[39m.\u001b[39;49mmake(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     91\u001b[0m \u001b[39m# We used to have people override _reset/_step rather than\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m# your environment if you use these methods and don't want\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m# compatibility code to be invoked.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(env, \u001b[39m\"\u001b[39m\u001b[39m_reset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(env, \u001b[39m\"\u001b[39m\u001b[39m_step\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(env, \u001b[39m\"\u001b[39m\u001b[39m_gym_disable_underscore_compat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym/envs/registration.py:60\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m load(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_point)\n\u001b[0;32m---> 60\u001b[0m     env \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs)\n\u001b[1;32m     62\u001b[0m \u001b[39m# Make the environment aware of which spec it came from.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m spec \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym_chess/__init__.py:32\u001b[0m, in \u001b[0;36m_make_env\u001b[0;34m(encode)\u001b[0m\n\u001b[1;32m     29\u001b[0m env \u001b[39m=\u001b[39m Chess()\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m encode:\n\u001b[0;32m---> 32\u001b[0m     env \u001b[39m=\u001b[39m BoardEncoding(env)\n\u001b[1;32m     33\u001b[0m     env \u001b[39m=\u001b[39m MoveEncoding(env)\n\u001b[1;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m env\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym_chess/alphazero/board_encoding.py:55\u001b[0m, in \u001b[0;36mBoardEncoding.__init__\u001b[0;34m(self, env, history_length)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, env, history_length: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39msuper\u001b[39m(BoardEncoding, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(env)\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_history \u001b[39m=\u001b[39m BoardHistory(history_length)\n\u001b[1;32m     57\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space \u001b[39m=\u001b[39m spaces\u001b[39m.\u001b[39mBox(\n\u001b[1;32m     58\u001b[0m         low\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     59\u001b[0m         high\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint)\u001b[39m.\u001b[39mmax,\n\u001b[1;32m     60\u001b[0m         shape\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m, history_length \u001b[39m*\u001b[39m \u001b[39m14\u001b[39m \u001b[39m+\u001b[39m \u001b[39m7\u001b[39m),\n\u001b[1;32m     61\u001b[0m         dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint\n\u001b[1;32m     62\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gym_chess/alphazero/board_encoding.py:131\u001b[0m, in \u001b[0;36mBoardHistory.__init__\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, length: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m     \u001b[39m#: Ring buffer of recent board encodings; stored boards are always\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[39m#: oriented towards the White player. \u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((length, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m14\u001b[39m), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mint)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[39m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"ChessAlphaZero-v0\"\n",
    ")  # We will use Alpha Zero's numenclature for the actions encodings\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "counter = 0  # Since each step represents a play in a chess game we are going to store the number of steps associated to the episode/game\n",
    "\n",
    "while not done:\n",
    "    if (\n",
    "        counter % 2 == 0\n",
    "    ):  # If the step number is pair, this means that it is the WHITE player's turn\n",
    "        action = WHITE_PLAYER_POLICY(env, state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "    else:  # If the step number is not pair, aka odd, this means that it is the BLACK player's turn\n",
    "        action = BLACK_PLAYER_POLICY(env, state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "print(reward)\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "#optimizing both with sarsa and time difference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The agent receives a reward of +1 when the white player makes a winning move, and a reward of -1 when the black player makes a winning move. \n",
    "\n",
    "All other rewards are zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluationg your agent with [Stockfish](https://github.com/zhelyabuzhsky/stockfish)\n",
    "\n",
    "In order to have a good enough idea that our agent is actually playing well we need a benchmarkable opponent.\n",
    "\n",
    "As such we need to install stockfish a free and open-source chess engine. Stockfish has consistently ranked first or near the top of most chess-engine rating lists and, as of April 2023, is the strongest CPU chess engine in the world.\n",
    "\n",
    "`pip install stockfish`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting stockfish\n",
      "  Downloading stockfish-3.28.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: stockfish\n",
      "Successfully installed stockfish-3.28.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install stockfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stockfish import Stockfish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StockFish has a python api as seen above, nevertheless the engine still needs to be downloaded [here](https://stockfishchess.org/download/) and used in the path.\n",
    "\n",
    "NOTE: You were given an engine already in moodle, nevertheless different computer systems (Windows, Mac, Ubuntu) might require other Stockfish engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stockfish_path = \"C:/Users/nunoa/Desktop/Aulas/Reinforcement Learning/FINAL PROJECT/stockfish_15.1_win_x64_avx2/stockfish-windows-2022-x86-64-avx2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions bellow generate episodes/games for a WHITE or BLACK Pieces Scenario respectively. We store the outcome of the episode (win/draw/loss) and the number of steps taken.\n",
    "\n",
    "#### Notice how the AGENT_POLICY function is used it recieves as inputs the env and the current state.\n",
    "`action = AGENT_POLICY(env, state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_WHITE_scenario(Stockfish_path, AGENT_POLICY):\n",
    "    env = gym.make(\n",
    "        \"ChessAlphaZero-v0\"\n",
    "    )  # We will use Alpha Zero's numenclature for the actions encodings\n",
    "    stockfish = Stockfish(Stockfish_path)\n",
    "    stockfish.set_elo_rating(\n",
    "        100\n",
    "    )  # Default \"skill\" level is 1350, higher will increase the skill of stockfish \"player\". See more at https://en.wikipedia.org/wiki/Elo_rating_system\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    counter = 0  # Since each step represents a play in a chess game we are going to store the number of steps associated to the episode/game\n",
    "\n",
    "    while not done:\n",
    "        if (\n",
    "            counter % 2 == 0\n",
    "        ):  # If the step number is pair, this means that it is the WHITE player's turn\n",
    "            action = AGENT_POLICY(env, state)\n",
    "            decoded_action = str(env.decode(action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        else:  # If the step number is not pair, aka odd, this means that it is the BLACK player's turn\n",
    "            decoded_action = stockfish.get_best_move()\n",
    "            action = env.encode(chess.Move.from_uci(decoded_action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return reward, np.ceil(counter / 2)\n",
    "\n",
    "\n",
    "def generate_BLACK_scenario(Stockfish_path, AGENT_POLICY):\n",
    "    env = gym.make(\n",
    "        \"ChessAlphaZero-v0\"\n",
    "    )  # We will use Alpha Zero's numenclature for the actions encodings\n",
    "    stockfish = Stockfish(Stockfish_path)\n",
    "    stockfish.set_elo_rating(\n",
    "        100\n",
    "    )  # Default \"skill\" level is 1350, higher will increase the skill of stockfish \"player\". See more at https://en.wikipedia.org/wiki/Elo_rating_system\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    counter = 0  # Since each step represents a play in a chess game we are going to store the number of steps associated to the episode/game\n",
    "\n",
    "    while not done:\n",
    "        if (\n",
    "            counter % 2 == 1\n",
    "        ):  # If the step number is not pair, aka odd, this means that it is the BLACK player's turn\n",
    "            action = AGENT_POLICY(env, state)\n",
    "            decoded_action = str(env.decode(action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        else:  # If the step number is pair, this means that it is the WHITE player's turn\n",
    "            decoded_action = stockfish.get_best_move()\n",
    "            action = env.encode(chess.Move.from_uci(decoded_action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return reward, np.ceil(counter / 2)\n",
    "#exploring the encoding of the chess enviorment "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function bellow a visualization is produced from the bechmarks made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AGENT_EVALUATION(Stockfish_path, AGENT_POLICY, n_evaluations=100):\n",
    "    results_list = []\n",
    "\n",
    "    for evaluation_number in tqdm(range(n_evaluations)):\n",
    "        generate_episode = generate_WHITE_scenario\n",
    "\n",
    "        reward, n_steps = generate_episode(Stockfish_path, AGENT_POLICY)\n",
    "\n",
    "        if reward == 1:\n",
    "            result = \"VICTORY\"\n",
    "        elif reward == 0:\n",
    "            result = \"DRAW\"\n",
    "        else:\n",
    "            result = \"LOSS\"\n",
    "\n",
    "        results_list.append([\"WHITE\", result, n_steps])\n",
    "\n",
    "        generate_episode = generate_BLACK_scenario\n",
    "\n",
    "        reward, n_steps = generate_episode(Stockfish_path, AGENT_POLICY)\n",
    "\n",
    "        if reward == -1:\n",
    "            result = \"VICTORY\"\n",
    "        elif reward == 0:\n",
    "            result = \"DRAW\"\n",
    "        else:\n",
    "            result = \"LOSS\"\n",
    "\n",
    "        results_list.append([\"BLACK\", result, n_steps])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        results_list, columns=[\"AGENT COLOR\", \"OUTCOME\", \"N STEPS\"]\n",
    "    ).astype(\"int\", errors=\"ignore\")\n",
    "\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "    results_group = (\n",
    "        df.groupby([\"AGENT COLOR\", \"OUTCOME\"])\n",
    "        .count()\n",
    "        .rename(columns={\"N STEPS\": \"GAMES\"})\n",
    "    )\n",
    "\n",
    "    n_games = results_group.sum()[0]\n",
    "\n",
    "    results_group = (2 * 100 * results_group / (n_games)).astype(\"int\")\n",
    "\n",
    "    viz_df = (\n",
    "        results_group.reset_index()\n",
    "        .pivot_table(index=\"AGENT COLOR\", columns=\"OUTCOME\", values=\"GAMES\")\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    viz_df.plot(kind=\"barh\", stacked=True)\n",
    "\n",
    "    plt.xlabel(\"Percentage\")\n",
    "    plt.title(f\"EVALUATION RESULTS FOR {n_games} GAMES\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = AGENT_EVALUATION(Stockfish_path, WHITE_PLAYER_POLICY, n_evaluations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready to Play Chess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can learning from other games (transfer learning)\n",
    "#for extra points (ok its optional but we can try it as extra)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
